{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6iNgfn2UNJYn"
   },
   "source": [
    "Import some tools in Pytorch.\n",
    "\n",
    "The nn means neural network and functional library has helpful built-in functions, check the library https://pytorch.org/docs/stable/nn.functional.html\n",
    "\n",
    "And the optim has optimizer such as gradient descent, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 6896,
     "status": "ok",
     "timestamp": 1711180588842,
     "user": {
      "displayName": "김규원",
      "userId": "06077094390316537514"
     },
     "user_tz": -540
    },
    "id": "-I39CaHENDmg"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ryyjnw8TNznE"
   },
   "source": [
    "Setting the dependent variables and independent variables as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "ok",
     "timestamp": 1711180588843,
     "user": {
      "displayName": "김규원",
      "userId": "06077094390316537514"
     },
     "user_tz": -540
    },
    "id": "f2eBR13jOC0n"
   },
   "outputs": [],
   "source": [
    "x_dep = torch.FloatTensor([[1], [2], [3]])\n",
    "y_indep = torch.FloatTensor([[2], [4], [6]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-Q2kh_FOLiC"
   },
   "source": [
    "Setting W and b as 0 and Clarifying that it is a variable whose value can be changed during learning.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1711180588843,
     "user": {
      "displayName": "김규원",
      "userId": "06077094390316537514"
     },
     "user_tz": -540
    },
    "id": "6P0EnzqNOK4A"
   },
   "outputs": [],
   "source": [
    "W = torch.zeros(1, requires_grad=True)\n",
    "b = torch.zeros(1, requires_grad=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E4ojahBiOiJe"
   },
   "source": [
    "Setting the hypothesis as linear function as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1711180588843,
     "user": {
      "displayName": "김규원",
      "userId": "06077094390316537514"
     },
     "user_tz": -540
    },
    "id": "klrIK2IPO7Of"
   },
   "outputs": [],
   "source": [
    "hypothesis = x_dep * W + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AD35XBpoPF1T"
   },
   "source": [
    "I will use stochastic gradient descent method and optimize the hypothesis function with 100 iterations\n",
    "\n",
    "Learning rate that I chosen here is 0.01.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4636,
     "status": "ok",
     "timestamp": 1711180593477,
     "user": {
      "displayName": "김규원",
      "userId": "06077094390316537514"
     },
     "user_tz": -540
    },
    "id": "533j8JW5PFHt",
    "outputId": "3ab769f6-4d81-4773-b2f9-154cea71c749"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch    0/100 W: 0.187, b: 0.080 Value: 18.666666\n",
      "Epoch   10/100 W: 1.224, b: 0.516 Value: 1.844294\n",
      "Epoch   20/100 W: 1.550, b: 0.638 Value: 0.239337\n",
      "Epoch   30/100 W: 1.655, b: 0.665 Value: 0.083519\n",
      "Epoch   40/100 W: 1.693, b: 0.662 Value: 0.065829\n",
      "Epoch   50/100 W: 1.709, b: 0.650 Value: 0.061424\n",
      "Epoch   60/100 W: 1.719, b: 0.636 Value: 0.058413\n",
      "Epoch   70/100 W: 1.726, b: 0.621 Value: 0.055656\n",
      "Epoch   80/100 W: 1.733, b: 0.607 Value: 0.053039\n",
      "Epoch   90/100 W: 1.739, b: 0.592 Value: 0.050546\n",
      "Epoch  100/100 W: 1.746, b: 0.578 Value: 0.048171\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.SGD([W, b], lr=0.01)\n",
    "nb_epochs = 100\n",
    "for epoch in range(nb_epochs + 1):\n",
    "    # H(x) calculation\n",
    "    hypothesis = x_dep * W + b\n",
    "\n",
    "    # value calculation\n",
    "    value = torch.mean((hypothesis - y_indep) ** 2)\n",
    "\n",
    "    # Using the gradient descent method, update the gradient\n",
    "    optimizer.zero_grad()\n",
    "    value.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # print a value every 10 times\n",
    "    if epoch % 10 == 0:\n",
    "        print(\n",
    "            \"Epoch {:4d}/{} W: {:.3f}, b: {:.3f} Value: {:.6f}\".format(\n",
    "                epoch, nb_epochs, W.item(), b.item(), value.item()\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3EFAlTBVQUxe"
   },
   "source": [
    "Here, the optimizer.zero_grad() is initializing the gradient as zero at every iterations.\n",
    "\n",
    "And value.backward calculates the gradient at the current W and b, and optimizer.step() update the W and b"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyMlE7R+Ac0fhM3I9HcXBO3r",
   "gpuType": "T4",
   "provenance": [
    {
     "file_id": "1sXmCNrCNWAZwR0U3L5_98DBcSLXJ1EMQ",
     "timestamp": 1711180678938
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}